{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shared1_TensorflowOnSpark1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "hLggCwJZkTjC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Overview \n",
        "\n",
        "Let's setup TensorflowOnSpark \n"
      ]
    },
    {
      "metadata": {
        "id": "Xz0xpi4CkZZZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Dependencies \n"
      ]
    },
    {
      "metadata": {
        "id": "4NQdu1mikIEB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c7eee6ad-911a-4683-e6ec-9e6abf28fe4c"
      },
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L-MCM_mckbG-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "d4009698-74e3-420c-f6d3-4fa0c5d81168"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflowonspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflowonspark in /usr/local/lib/python3.6/dist-packages (1.4.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from tensorflowonspark) (1.13.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflowonspark) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflowonspark) (1.13.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflowonspark) (3.6.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflowonspark) (1.0.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflowonspark) (1.11.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflowonspark) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflowonspark) (0.7.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflowonspark) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflowonspark) (1.0.9)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflowonspark) (1.13.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflowonspark) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflowonspark) (1.14.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflowonspark) (0.7.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->tensorflowonspark) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow->tensorflowonspark) (40.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow->tensorflowonspark) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow->tensorflowonspark) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow->tensorflowonspark) (3.0.1)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->tensorflowonspark) (5.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5RMe3daqkgtM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Spark Context \n",
        "\n",
        "In order to be able to interact with Spark inside the Notebook, we need the context which we can get with this API \n"
      ]
    },
    {
      "metadata": {
        "id": "prTvTxSckc0D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.conf import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QtJvDhwkkqCi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sc=SparkContext.getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-BHJvwhukrpj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "e2220898-e12e-4edd-9138-ed53c24abc7c"
      },
      "cell_type": "code",
      "source": [
        "sc"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://480eba8ba606:4041\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "SbzT6r5VktO9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "55c71e58-07dd-498e-ffdf-4541232d5908"
      },
      "cell_type": "code",
      "source": [
        "num_executors = sc.defaultParallelism\n",
        "num_executors"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "3Z8paGsdlyww",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# TensorflowOnSpark \n",
        "\n",
        "- Create a Cluster using the SparkContent and run training via that interface \n",
        "\n",
        "- The Cluster is created with [TFCluster API](https://www.pydoc.io/pypi/tensorflowonspark-1.0.7/autoapi/TFCluster/index.html) calling the `run()` method \n",
        "\n",
        "```python\n",
        "cluster = TFCluster.run(sc, map_fn, args, num_executors, num_ps, tensorboard, input_mode)\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "rqD6G9rwqacm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Arguments \n",
        "\n",
        "- From example taken from [Distributed MNIST](https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/mnist_spark.ipynb)\n"
      ]
    },
    {
      "metadata": {
        "id": "9rv0IsuoqjsL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4VLlT7tuluGs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "385e535f-fa7d-4e18-a76b-10c1482ad333"
      },
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--batch_size\", help=\"number of examples per batch\", type=int, default=100)\n",
        "parser.add_argument(\"--epochs\", help=\"number of epochs\", type=int, default=1)\n",
        "parser.add_argument(\"--format\", help=\"example format\", choices=[\"csv\",\"pickle\",\"tfr\"], default=\"csv\")\n",
        "parser.add_argument(\"--images\", help=\"HDFS path to MNIST images in parallelized format\")\n",
        "parser.add_argument(\"--labels\", help=\"HDFS path to MNIST labels in parallelized format\")\n",
        "parser.add_argument(\"--mode\", help=\"train|inference\", default=\"train\")\n",
        "parser.add_argument(\"--model\", help=\"HDFS path to save/load model during train/test\", default=\"mnist_model\")\n",
        "parser.add_argument(\"--output\", help=\"HDFS path to save test/inference output\", default=\"predictions\")\n",
        "parser.add_argument(\"--readers\", help=\"number of reader/enqueue threads\", type=int, default=1)\n",
        "parser.add_argument(\"--rdma\", help=\"use rdma connection\", default=False)\n",
        "parser.add_argument(\"--steps\", help=\"maximum number of steps\", type=int, default=1000)\n",
        "parser.add_argument(\"--tensorboard\", help=\"launch tensorboard process\", action=\"store_true\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreTrueAction(option_strings=['--tensorboard'], dest='tensorboard', nargs=0, const=True, default=False, type=None, choices=None, help='launch tensorboard process', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "iGFWOwjxqbxv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}

